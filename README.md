# S3 Orchestrator

[![CI](https://github.com/afreidah/s3-orchestrator/actions/workflows/ci.yml/badge.svg)](https://github.com/afreidah/s3-orchestrator/actions/workflows/ci.yml)
[![codecov](https://codecov.io/gh/afreidah/s3-orchestrator/branch/main/graph/badge.svg)](https://codecov.io/gh/afreidah/s3-orchestrator)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

An S3-compatible orchestrator that combines multiple storage backends into a single unified endpoint. Add as many S3-compatible backends as you want — OCI Object Storage, Backblaze B2, AWS S3, MinIO, whatever — and the orchestrator presents them to clients as one or more virtual buckets. Per-backend quota enforcement lets you cap each backend at exactly the byte limit you choose, so you can stack multiple free-tier allocations from different providers into a single, larger storage target for backups, media, or anything else without worrying about surprise bills.

Multiple virtual buckets let different applications share the same orchestrator with isolated file namespaces and independent credentials. Each bucket's objects are stored with an internal key prefix (`{bucket}/{key}`), so bucket isolation requires zero changes to the storage layer or database schema.

Built-in cross-backend replication also makes this an easy way to keep your data in multiple clouds without touching your application. Point your app at the proxy, set a replication factor, and every object automatically lands in two or more providers — instant multi-cloud redundancy with zero client-side changes.

Objects are routed to backends based on the configured `routing_strategy`: **pack** (default) fills backends in config order, while **spread** places each write on the least-utilized backend by ratio. Metadata and quota tracking live in PostgreSQL; the backends only see standard S3 API calls. The orchestrator is fully S3-compatible and works with any standard S3 client.

![Dashboard](docs/images/dashboard.png?v=2)

## Architecture

```
              S3 clients (aws cli, rclone, etc.)
                          |
                          v
                    +-----------+
                    | S3 Orch.  |  <-- SigV4 auth, rate limiting, quota routing
                    +-----------+
                     |         |
            +--------+         +------------------+------------------+
            v                  v                  v                  v
       PostgreSQL        OCI Object         Backblaze B2          AWS S3
       (metadata)       Storage (20 GB)       (10 GB)             (5 GB)
                              \                  |                  /
                               '------------ 35 GB total ---------'
```

- **PostgreSQL** stores object locations (`object_locations`), per-backend quota counters (`backend_quotas`), and multipart upload state (`multipart_uploads`, `multipart_parts`). Schema is applied automatically on startup via embedded SQL. All queries are generated by [sqlc](https://sqlc.dev/) from annotated SQL files and executed via [pgx/v5](https://github.com/jackc/pgx) connection pools.
- **Backends** are standard S3-compatible services accessed via AWS SDK v2. Any provider that speaks the S3 API works — OCI Object Storage, Backblaze B2, AWS S3, MinIO, Wasabi, etc.
- **Write routing** selects a backend for each new object based on the `routing_strategy`. In **pack** mode (default), objects go to the first backend in config order that has available quota — good for filling free-tier allocations sequentially. In **spread** mode, objects go to the backend with the lowest utilization ratio (`bytes_used / bytes_limit`) — good for distributing load evenly across backends. Quota is updated atomically in a transaction alongside the object location record. Set `quota_bytes: 0` (or omit it) to disable quota enforcement on a backend — useful when you don't need cost control and just want unified access or replication.
- **Usage limits** optionally cap monthly API requests, egress bytes, and ingress bytes per backend. When a backend exceeds a limit, writes overflow to other backends and reads fail over to replicas. Delete and abort operations always bypass limits. Limits are enforced using cached database totals (refreshed at the configured flush interval) plus unflushed in-memory counters. Adaptive flushing automatically shortens the interval when any backend approaches a limit.

## S3 API Coverage

| Operation | Method | Path | Notes |
|-----------|--------|------|-------|
| PutObject | `PUT` | `/{bucket}/{key}` | |
| GetObject | `GET` | `/{bucket}/{key}` | Supports `Range` header (206 Partial Content) |
| HeadObject | `HEAD` | `/{bucket}/{key}` | |
| DeleteObject | `DELETE` | `/{bucket}/{key}` | Idempotent (404 from store treated as success) |
| DeleteObjects | `POST` | `/{bucket}?delete` | Batch delete up to 1000 keys per request |
| CopyObject | `PUT` | `/{bucket}/{key}` | Uses `X-Amz-Copy-Source` header |
| ListObjectsV2 | `GET` | `/{bucket}?list-type=2` | Supports `delimiter` for virtual directories |
| CreateMultipartUpload | `POST` | `/{bucket}/{key}?uploads` | |
| UploadPart | `PUT` | `/{bucket}/{key}?partNumber=N&uploadId=X` | |
| CompleteMultipartUpload | `POST` | `/{bucket}/{key}?uploadId=X` | |
| AbortMultipartUpload | `DELETE` | `/{bucket}/{key}?uploadId=X` | |
| ListParts | `GET` | `/{bucket}/{key}?uploadId=X` | |

**Batch delete** (`DeleteObjects`) accepts an XML request body listing up to 1000 keys and returns per-key success/error results. Metadata removal is sequential (each key is its own DB transaction), while backend S3 deletes run concurrently with bounded parallelism for throughput. Failed backend deletes are enqueued to the cleanup queue for automatic retry. The response always returns HTTP 200, even when individual keys fail -- errors are reported per-key in the XML body. Quiet mode (`<Quiet>true</Quiet>`) suppresses the `<Deleted>` elements and only returns errors.

Each request must target a virtual bucket name that matches the credentials used to sign the request. Requests to a bucket the credentials aren't authorized for return `403 AccessDenied`.

Every response includes an `X-Amz-Request-Id` header with a unique request ID for tracing. Clients can supply their own ID via `X-Request-Id`; otherwise the orchestrator generates one. The same ID appears in audit logs and OpenTelemetry spans.

## Authentication & Multi-Bucket

Each virtual bucket has one or more credential sets. On every request, the orchestrator:

1. Extracts the access key from the SigV4 `Authorization` header (or token from `X-Proxy-Token`).
2. Looks up which bucket the credential belongs to.
3. Verifies the signature (SigV4) or token.
4. Validates the URL path bucket matches the authorized bucket.

Two auth methods are supported, checked in order:

1. **AWS SigV4** (recommended) - Standard AWS Signature Version 4 via the `Authorization` header. Compatible with `aws cli`, SDKs, and any S3 client.
2. **Legacy token** - Simple `X-Proxy-Token` header for backward compatibility.

Multiple services can share a bucket by each having their own credentials that all map to the same bucket name. Access key IDs must be globally unique across all buckets.

Authentication is always required — every bucket must have at least one credential set.

For client usage examples (AWS CLI, rclone, boto3, Go SDK), see the [User Guide](docs/user-guide.md).
For deployment and operations, see the [Admin Guide](docs/admin-guide.md).

## Degraded Mode (Circuit Breaker)

A three-state circuit breaker wraps all database access:

```
closed (healthy) → open (DB down) → half-open (probing) → closed
```

When the database becomes unreachable (consecutive failures exceed `failure_threshold`), the orchestrator enters **degraded mode**:

- **Reads** broadcast to all backends in order (or in parallel if `parallel_broadcast` is enabled). A location cache (TTL configurable via `cache_ttl`) stores successful lookups to avoid repeated broadcasts for the same key.
- **Writes** (PUT, DELETE, COPY, multipart) return `503 ServiceUnavailable`.
- **Health endpoint** returns `degraded` instead of `ok`.

After `open_timeout` elapses, the circuit enters half-open state and sends a single probe request. If the database responds, the circuit closes and normal operation resumes automatically.

## Write Routing

The `routing_strategy` setting controls how the orchestrator selects a backend for new objects (PutObject, CopyObject, CreateMultipartUpload):

- **pack** (default) — fills the first backend in config order until its quota is full, then overflows to the next. Good for maximizing usable capacity on free-tier providers where you want to fill one allocation before touching the next.
- **spread** — places each object on the backend with the lowest utilization ratio (`bytes_used / bytes_limit`). Good for distributing storage evenly across backends to balance load and wear.

Both strategies respect quota limits — a backend with no remaining space is skipped regardless of strategy. When usage limits are configured, backends that have exceeded their monthly limits are also excluded from selection.

## Rebalancing

The rebalancer periodically moves objects between backends to optimize storage distribution. Disabled by default to avoid unexpected egress charges.

Two strategies:

- **pack** - Fills backends in configuration order, consolidating free space on the last backend. Good for maximizing usable capacity on free-tier providers.
- **spread** - Equalizes utilization ratios across all backends. Good for distributing load evenly.

The `threshold` parameter (0–1) sets the minimum utilization spread required to trigger a rebalance run. Objects are moved in configurable batch sizes with bounded concurrency (`concurrency` setting, default 5) for throughput.

## Replication

When `replication.factor` is greater than 1, a background worker creates additional copies of objects on different backends to reach the target factor. Read operations automatically fail over to replicas if the primary copy is unavailable.

The worker runs once at startup to catch up on pending replicas, then continues at the configured interval.

## Cleanup Queue

When a backend S3 operation succeeds but the subsequent metadata update or cleanup deletion fails, an orphaned object is left on the backend — invisible to the system, consuming storage but not tracked by quotas. Rather than silently logging these failures, the orchestrator enqueues them in a persistent `cleanup_queue` table in PostgreSQL for automatic retry.

A background worker runs every minute, fetching pending items and attempting to delete them from their respective backends. Failed attempts are rescheduled with exponential backoff (`1m × 2^attempts`, capped at 24h). After 10 failed attempts, the item is left in the table for manual inspection — the worker query filters it out automatically via a partial index.

Enqueue points cover all failure sites across the codebase:

- **PutObject / CopyObject / CompleteMultipartUpload** — orphaned object when `RecordObject` fails and the immediate cleanup delete also fails
- **DeleteObject** — metadata removed but backend delete fails (storage leak)
- **UploadPart** — part uploaded but `RecordPart` fails and cleanup delete fails
- **CompleteMultipartUpload / AbortMultipartUpload** — temporary `__multipart/` part objects not deleted
- **Rebalancer** — orphaned copy on destination when `MoveObjectLocation` fails, or stale source copy after a successful move
- **Replicator** — orphaned replica when `RecordReplica` fails or source is deleted during replication

Enqueue is best-effort: if the database is down (circuit breaker open), the failure is logged and the orphan is not enqueued. This avoids cascading failures — if the DB recovers, the next operation that fails will be enqueued normally.

Operators can inspect exhausted items directly:

```sql
SELECT * FROM cleanup_queue WHERE attempts >= 10;
```

## Lifecycle (Object Expiration)

Config-driven lifecycle rules automatically delete objects matching a key prefix after a configurable number of days. Useful for expiring temporary uploads, staging artifacts, or any objects with a known retention period.

```yaml
lifecycle:
  rules:
    - prefix: "tmp/"
      expiration_days: 7
    - prefix: "uploads/staging/"
      expiration_days: 1
```

A background worker runs hourly and evaluates each rule against `created_at` timestamps in the `object_locations` table (uses an existing index — no schema changes needed). Deletions go through the standard `DeleteObject` path, so all copies are removed, quotas are decremented, and failed backend deletes are enqueued to the cleanup queue.

Rules are hot-reloadable via `SIGHUP`. An empty rules list (or omitting the section entirely) disables lifecycle — no advisory lock is acquired and no DB queries are executed.

## Rate Limiting

Optional per-IP token bucket rate limiting. When enabled, requests exceeding the configured rate return `429 SlowDown`. Stale IP entries are cleaned up automatically.

When running behind a reverse proxy (e.g., Traefik, nginx), configure `trusted_proxies` with the proxy's CIDR ranges so the orchestrator extracts the real client IP from the `X-Forwarded-For` header using rightmost-untrusted extraction. Without `trusted_proxies`, `X-Forwarded-For` is ignored and the direct connection IP is always used.

## Usage Limits

Per-backend monthly limits for API requests, egress bytes, and ingress bytes. Set any limit to `0` (or omit it) for unlimited. Limits reset naturally each month — the usage tracking table is keyed by `YYYY-MM` period.

**Enforcement behavior:**

- **Writes** (PutObject, CopyObject, CreateMultipartUpload, UploadPart) — backends over their limits are excluded from selection; writes overflow to the next eligible backend. If all backends are over-limit, the orchestrator returns `507 InsufficientStorage`.
- **Reads** (GetObject, HeadObject) — over-limit backends are skipped; the orchestrator tries replicas. Returns `429 SlowDown` only when *all* copies of the object are on over-limit backends.
- **Deletes** (DeleteObject, DeleteObjects, AbortMultipartUpload) — always allowed regardless of limits.

Effective usage is computed as `DB baseline + unflushed in-memory counters + proposed operation`, so enforcement stays accurate between flush/refresh cycles without double-counting. The flush interval is configurable (default 30s) and can adaptively shorten when backends approach their limits.

## Configuration

YAML config file specified via `-config` flag (default: `config.yaml`). Supports `${ENV_VAR}` expansion.

```yaml
server:
  listen_addr: "0.0.0.0:9000"
  max_object_size: 5368709120  # 5 GB (default)

# Virtual buckets with per-bucket credentials
buckets:
  - name: "app1-files"
    credentials:
      - access_key_id: "APP1_ACCESS_KEY"
        secret_access_key: "APP1_SECRET_KEY"

  - name: "shared-files"
    credentials:
      # Multiple services can share a bucket with separate credentials
      - access_key_id: "WRITER_ACCESS_KEY"
        secret_access_key: "WRITER_SECRET_KEY"
      - access_key_id: "READER_ACCESS_KEY"
        secret_access_key: "READER_SECRET_KEY"

  # Legacy token auth (backward compatibility)
  # - name: "legacy-bucket"
  #   credentials:
  #     - token: "my-secret-token"

database:
  host: "localhost"
  port: 5432
  database: "s3proxy"
  user: "s3proxy"
  password: "secret"
  ssl_mode: "require"
  max_conns: 10
  min_conns: 5
  max_conn_lifetime: "5m"

routing_strategy: "pack"       # "pack" (fill in order) or "spread" (least utilized) (default: pack)

backends:
  - name: "oci"
    endpoint: "https://namespace.compat.objectstorage.region.oraclecloud.com"
    region: "us-phoenix-1"
    bucket: "my-bucket"
    access_key_id: "backend-access-key"
    secret_access_key: "backend-secret-key"
    force_path_style: true
    unsigned_payload: true    # stream uploads without buffering (default: true)
    quota_bytes: 21474836480  # 20 GB (0 or omit for unlimited)
    api_request_limit: 0      # monthly API request limit (0 = unlimited)
    egress_byte_limit: 0      # monthly egress byte limit (0 = unlimited)
    ingress_byte_limit: 0     # monthly ingress byte limit (0 = unlimited)

telemetry:
  metrics:
    enabled: true
    path: "/metrics"
  tracing:
    enabled: true
    endpoint: "localhost:4317"
    insecure: true
    sample_rate: 1.0

circuit_breaker:
  failure_threshold: 3     # consecutive DB failures before opening (default: 3)
  open_timeout: "15s"      # delay before probing recovery (default: 15s)
  cache_ttl: "60s"         # key→backend cache TTL during degraded reads (default: 60s)
  parallel_broadcast: false # fan-out reads to all backends in parallel during degraded mode (default: false)

rebalance:
  enabled: false
  strategy: "pack"         # "pack" or "spread" (default: pack)
  interval: "6h"           # run interval (default: 6h)
  batch_size: 100          # max objects per run (default: 100)
  threshold: 0.1           # min utilization spread to trigger (default: 0.1)
  concurrency: 5           # parallel moves per run (default: 5)

replication:
  factor: 1                # copies per object; 1 = no replication (default: 1)
  worker_interval: "5m"    # replication worker cycle (default: 5m)
  batch_size: 50           # objects per cycle (default: 50)

rate_limit:
  enabled: false
  requests_per_sec: 100    # token refill rate (default: 100)
  burst: 200               # max burst size (default: 200)
  # trusted_proxies:       # CIDRs whose X-Forwarded-For is trusted
  #   - "10.0.0.0/8"       # Uses rightmost-untrusted extraction
  #   - "172.16.0.0/12"

ui:
  enabled: false             # enable the built-in web dashboard
  path: "/ui"                # URL prefix (default: /ui)
  admin_key: "${UI_ADMIN_KEY}"       # access key for dashboard login
  admin_secret: "${UI_ADMIN_SECRET}" # secret key for dashboard login

usage_flush:
  interval: "30s"            # base flush interval (default: 30s)
  adaptive_enabled: false    # shorten interval when near usage limits (default: false)
  adaptive_threshold: 0.8    # usage ratio to trigger fast flush (default: 0.8)
  fast_interval: "5s"        # interval when near limits (default: 5s)

lifecycle:
  rules:                       # empty or omitted = lifecycle disabled
    - prefix: "tmp/"           # key prefix to match
      expiration_days: 7       # delete objects older than this
    - prefix: "uploads/staging/"
      expiration_days: 1
```

## Configuration Hot-Reload

The orchestrator supports hot-reloading a subset of configuration by sending `SIGHUP` to the running process. This lets you update credentials, quotas, rate limits, and other operational settings without restarting the service or dropping client connections.

```bash
kill -HUP $(pidof s3-orchestrator)
```

### Reloadable vs non-reloadable settings

| Setting | Reloadable | Notes |
|---------|:----------:|-------|
| `buckets` (credentials) | Yes | New credentials take effect immediately |
| `rate_limit` | Yes | New visitors get updated rates; existing per-IP limiters expire naturally |
| `backends[].quota_bytes` | Yes | Synced to database on reload |
| `backends[].api_request_limit` | Yes | |
| `backends[].egress_byte_limit` | Yes | |
| `backends[].ingress_byte_limit` | Yes | |
| `rebalance` | Yes | Strategy, interval, threshold, concurrency, enabled/disabled |
| `replication` | Yes | Factor, worker interval, batch size |
| `usage_flush` | Yes | Interval, adaptive enabled/threshold/fast interval |
| `lifecycle` | Yes | Rules (prefix, expiration_days) |
| `server.listen_addr` | No | Requires restart |
| `database` | No | Requires restart |
| `telemetry` | No | Requires restart |
| `circuit_breaker` | No | Requires restart |
| `ui` | No | Requires restart |
| `routing_strategy` | No | Requires restart |
| `backends` (structural: endpoint, credentials, count) | No | Requires restart |

On a successful reload, the orchestrator logs each reloaded section:

```
{"level":"INFO","msg":"SIGHUP received, reloading configuration","path":"config.yaml"}
{"level":"INFO","msg":"Reloaded bucket credentials","buckets":2}
{"level":"INFO","msg":"Reloaded rate limits","requests_per_sec":100,"burst":200}
{"level":"INFO","msg":"Reloaded backend quota limits"}
{"level":"INFO","msg":"Reloaded backend usage limits"}
{"level":"INFO","msg":"Reloaded rebalance/replication/usage-flush config"}
{"level":"INFO","msg":"Configuration reload complete"}
```

If the new config file is invalid, the orchestrator keeps the current configuration and logs the error:

```
{"level":"ERROR","msg":"Config reload failed, keeping current config","error":"invalid config: ..."}
```

Non-reloadable field changes are logged as warnings but do not prevent the reload of other settings:

```
{"level":"WARN","msg":"Config field changed but requires restart to take effect","field":"server.listen_addr"}
```

## Database

The orchestrator connects to PostgreSQL via pgx/v5 connection pools and auto-applies its schema on startup (all DDL uses `IF NOT EXISTS`). Four tables are created:

| Table | Purpose |
|-------|---------|
| `backend_quotas` | Per-backend byte limits and usage counters |
| `object_locations` | Maps object keys to backends with size tracking |
| `multipart_uploads` | In-progress multipart upload metadata |
| `multipart_parts` | Individual parts for active multipart uploads |
| `backend_usage` | Monthly per-backend API request and data transfer counters |
| `cleanup_queue` | Retry queue for failed backend object deletions |

Quota updates are transactional: object location inserts/deletes and quota counter changes happen atomically.

All SQL queries live in `internal/storage/sqlc/queries/` as annotated `.sql` files. Type-safe Go code is generated by sqlc into `internal/storage/sqlc/`. To regenerate after editing queries:

```bash
make generate
```

## Telemetry

### Prometheus Metrics

All metrics are prefixed with `s3proxy_`. Exposed at `/metrics` when enabled.

| Metric | Type | Labels | Description |
|--------|------|--------|-------------|
| `s3proxy_build_info` | Gauge | version, go_version | Build metadata |
| `s3proxy_requests_total` | Counter | method, status_code | HTTP request count |
| `s3proxy_request_duration_seconds` | Histogram | method | Request latency |
| `s3proxy_request_size_bytes` | Histogram | method | Upload sizes |
| `s3proxy_response_size_bytes` | Histogram | method | Download sizes |
| `s3proxy_inflight_requests` | Gauge | method | Currently processing |
| `s3proxy_backend_requests_total` | Counter | operation, backend, status | Backend S3 API calls |
| `s3proxy_backend_duration_seconds` | Histogram | operation, backend | Backend latency |
| `s3proxy_manager_requests_total` | Counter | operation, backend, status | Manager-level operations |
| `s3proxy_manager_duration_seconds` | Histogram | operation, backend | Manager latency |
| `s3proxy_quota_bytes_used` | Gauge | backend | Current bytes used |
| `s3proxy_quota_bytes_limit` | Gauge | backend | Quota limit |
| `s3proxy_quota_bytes_available` | Gauge | backend | Remaining space |
| `s3proxy_objects_count` | Gauge | backend | Stored object count |
| `s3proxy_active_multipart_uploads` | Gauge | backend | In-progress uploads |
| `s3proxy_rebalance_objects_moved_total` | Counter | strategy, status | Objects moved by rebalancer |
| `s3proxy_rebalance_bytes_moved_total` | Counter | strategy | Bytes moved by rebalancer |
| `s3proxy_rebalance_runs_total` | Counter | strategy, status | Rebalancer executions |
| `s3proxy_rebalance_duration_seconds` | Histogram | strategy | Rebalancer execution time |
| `s3proxy_rebalance_skipped_total` | Counter | reason | Rebalancer runs skipped |
| `s3proxy_replication_pending` | Gauge | — | Objects below replication factor |
| `s3proxy_replication_copies_created_total` | Counter | — | Replica copies created |
| `s3proxy_replication_errors_total` | Counter | — | Replication errors |
| `s3proxy_replication_duration_seconds` | Histogram | — | Replication cycle time |
| `s3proxy_replication_runs_total` | Counter | status | Replication worker executions |
| `s3proxy_circuit_breaker_state` | Gauge | — | 0=closed, 1=open, 2=half-open |
| `s3proxy_circuit_breaker_transitions_total` | Counter | from, to | State transitions |
| `s3proxy_degraded_reads_total` | Counter | operation | Broadcast reads in degraded mode |
| `s3proxy_degraded_cache_hits_total` | Counter | — | Cache hits during degraded reads |
| `s3proxy_degraded_write_rejections_total` | Counter | operation | Writes rejected in degraded mode |
| `s3proxy_usage_api_requests` | Gauge | backend | Current month API request count |
| `s3proxy_usage_egress_bytes` | Gauge | backend | Current month egress bytes |
| `s3proxy_usage_ingress_bytes` | Gauge | backend | Current month ingress bytes |
| `s3proxy_usage_limit_rejections_total` | Counter | operation, limit_type | Operations rejected by usage limits |
| `s3proxy_cleanup_queue_enqueued_total` | Counter | reason | Items added to the cleanup retry queue |
| `s3proxy_cleanup_queue_processed_total` | Counter | status | Items processed from the cleanup queue (success/retry/exhausted) |
| `s3proxy_cleanup_queue_depth` | Gauge | — | Current pending items in the cleanup queue |
| `s3proxy_rate_limit_rejections_total` | Counter | — | Requests rejected by per-IP rate limiting |
| `s3proxy_lifecycle_deleted_total` | Counter | — | Objects deleted by lifecycle expiration |
| `s3proxy_lifecycle_failed_total` | Counter | — | Objects that failed lifecycle deletion |
| `s3proxy_lifecycle_runs_total` | Counter | status | Lifecycle worker executions |
| `s3proxy_audit_events_total` | Counter | event | Audit log entries emitted |

Quota metrics are refreshed from PostgreSQL every 30 seconds (no backend API calls).

A ready-to-import Grafana dashboard covering all metrics is included at `grafana/s3-orchestrator.json`.

### OpenTelemetry Tracing

Spans are emitted for every HTTP request, manager operation, and backend S3 call. The service registers as `s3-orchestrator` (`resource.service.name`). Traces propagate via W3C `traceparent` headers. Configured to export via gRPC OTLP to Tempo or any OTLP-compatible collector.

### Audit Logging

Structured audit log entries are emitted as JSON via `slog` for every S3 API request and significant internal operation. Each entry includes an `"audit": true` marker for easy filtering in log pipelines.

**Request ID tracing** — every S3 API request gets a unique request ID, returned in the `X-Amz-Request-Id` response header. Clients can supply their own via the `X-Request-Id` request header. The same ID flows through context to all downstream operations, appearing in both the HTTP-level audit entry and the storage-level audit entry for full request correlation. The ID is also set as a `s3proxy.request_id` attribute on OpenTelemetry spans, linking audit logs to traces.

**Two-level audit entries** — each S3 request produces two audit log lines: one at the HTTP layer (`s3.PutObject`, `s3.GetObject`, etc.) with method, path, bucket, status, duration, and remote address, and one at the storage layer (`storage.PutObject`, `storage.GetObject`, etc.) with the backend name, object key, and size. Both share the same `request_id`.

**Internal operation auditing** — background operations generate their own correlation IDs:

| Operation | Events |
|-----------|--------|
| Rebalancer | `rebalance.start`, `rebalance.move`, `rebalance.complete` |
| Replicator | `replication.start`, `replication.copy`, `replication.complete` |
| Multipart cleanup | `storage.MultipartCleanup` |
| Cleanup queue | `cleanup_queue.processed` |

Example audit log entry:

```json
{"level":"INFO","msg":"audit","audit":true,"event":"s3.PutObject","request_id":"a1b2c3d4e5f6...","operation":"PutObject","method":"PUT","path":"/my-files/photo.jpg","bucket":"my-files","status":200,"duration":"45ms"}
```

## Web UI

A built-in web dashboard provides operational visibility and management without external tooling. When enabled, it renders a server-side HTML page at the configured path (default `/ui/`). All routes require authentication via HMAC-signed session cookies — users log in with an admin key/secret pair configured in the YAML config.

The dashboard shows:

- **Storage Summary** — total bytes used/capacity across all backends with a progress bar
- **Backends** — quota used/limit per backend with progress bars, object counts, active multipart uploads
- **Monthly Usage** — API requests, egress, and ingress per backend with limits
- **Objects** — interactive collapsible tree browser; buckets and directories expand on click to reveal contents, with rollup file counts and sizes
- **Configuration** — virtual buckets, write routing strategy, replication factor, rebalance strategy, rate limit status

The dashboard also provides management actions:

- **Upload** — upload files to any virtual bucket via the browser
- **Delete** — delete individual objects from the file tree
- **Rebalance** — trigger an on-demand rebalance across backends
- **Sync** — import pre-existing objects from a backend's S3 bucket into the proxy database, scoped to a selected virtual bucket

The object tree uses JavaScript for lazy-loaded AJAX expansion — directories load their children on click via the `/ui/api/tree` endpoint. All dashboard responses include security headers (`X-Frame-Options`, `X-Content-Type-Options`, `Referrer-Policy`, `Content-Security-Policy`). Enable it in the config:

```yaml
ui:
  enabled: true
  path: "/ui"                # default
  admin_key: "${UI_ADMIN_KEY}"
  admin_secret: "${UI_ADMIN_SECRET}"
```

JSON APIs are available at `{path}/api/dashboard` and `{path}/api/tree` for programmatic access. Management endpoints (`{path}/api/delete`, `{path}/api/upload`, `{path}/api/rebalance`, `{path}/api/sync`) accept POST requests and return JSON responses.

## Endpoints

| Path | Purpose |
|------|---------|
| `/health` | Health check — returns `ok` or `degraded` (always 200) |
| `/metrics` | Prometheus metrics |
| `/ui/` | Web dashboard (when enabled) |
| `/ui/login` | Dashboard login page |
| `/ui/api/dashboard` | Dashboard data as JSON |
| `/ui/api/tree` | Lazy-loaded directory listing as JSON |
| `/ui/api/delete` | Delete an object (POST, JSON body) |
| `/ui/api/upload` | Upload a file (POST, multipart form) |
| `/ui/api/rebalance` | Trigger on-demand rebalance (POST) |
| `/ui/api/sync` | Import objects from a backend (POST, JSON body) |
| `/{bucket}/{key}` | S3 API |

## Background Tasks

| Task | Interval | Advisory Lock | Description |
|------|----------|:-------------:|-------------|
| Usage flush + metrics | configurable (default 30s) | No | Flushes in-memory usage counters to PostgreSQL, then refreshes quota stats, usage baselines, object counts, and multipart counts. Updates Prometheus gauges. Adaptive mode shortens interval near limits. |
| Stale multipart cleanup | 1h | Yes | Aborts multipart uploads older than 24h and deletes their temporary part objects. |
| Cleanup queue | 1m | Yes | Retries failed backend object deletions with exponential backoff (1m to 24h, max 10 attempts). |
| Rebalancer | configurable (default 6h) | Yes | Moves objects between backends per strategy. Only runs when enabled. |
| Replicator | configurable (default 5m) | Yes | Creates copies of under-replicated objects. Only runs when factor > 1. Runs once at startup. |
| Lifecycle | 1h | Yes | Deletes objects matching lifecycle rules whose `created_at` exceeds `expiration_days`. Only runs when rules are configured. |

## Multi-Instance Deployment

Multiple orchestrator instances can safely share the same PostgreSQL database. Background tasks (rebalancer, replicator, cleanup queue, multipart cleanup) use PostgreSQL advisory locks to prevent concurrent execution across instances — if one instance holds the lock for a task, other instances skip that tick silently.

Request-serving paths (PutObject, GetObject, etc.) are stateless and work correctly with any number of instances behind a load balancer. The per-instance location cache is TTL-bounded and self-correcting. Rate limiting remains per-instance.

Usage flush counters are tracked in-memory per instance and flushed to the database at the configured interval. The adaptive flushing feature shortens the interval when any backend approaches a usage limit, improving enforcement accuracy in multi-instance setups.

## Sync Subcommand

Imports pre-existing objects from a backend S3 bucket into the orchestrator's metadata database. Useful when bringing an existing bucket under orchestrator management. The `--bucket` flag specifies which virtual bucket the imported objects belong to — keys are stored with a `{bucket}/` prefix for namespace isolation.

```bash
# Import all objects from a backend into the "unified" virtual bucket
s3-orchestrator sync --config config.yaml --backend oci --bucket unified

# Preview what would be imported
s3-orchestrator sync --config config.yaml --backend oci --bucket unified --dry-run

# Import only objects under a prefix
s3-orchestrator sync --config config.yaml --backend oci --bucket unified --prefix photos/
```

| Flag | Default | Description |
|------|---------|-------------|
| `--config` | `config.yaml` | Path to configuration file |
| `--backend` | (required) | Backend name to sync |
| `--bucket` | (required) | Virtual bucket name to prefix imported keys with |
| `--prefix` | `""` | Only sync objects with this key prefix |
| `--dry-run` | `false` | Preview what would be imported without writing |

Objects already tracked in the database for that backend are skipped. The command logs per-page progress and a final summary with imported count, skipped count, and total bytes imported.

## Development

```bash
# Install build and packaging dependencies
make tools

# Regenerate sqlc query code (after editing .sql files)
make generate

# Run locally (requires PostgreSQL and config.yaml)
make run

# Lint
make lint

# Static analysis
make vet

# Run unit tests
make test

# Run integration tests (requires Docker)
make integration-test

# Build local Docker image
make build

# Build multi-arch and push to registry
make push VERSION=vX.Y.Z

# Build a .deb package for the host architecture
make deb VERSION=X.Y.Z

# Build .deb packages for both amd64 and arm64
make deb-all VERSION=X.Y.Z

# Build and run lintian validation
make deb-lint VERSION=X.Y.Z

# Dry-run GoReleaser locally (builds everything without publishing)
make release-local
```

## Deployment

The orchestrator can run as a Docker container or as a native systemd service.

### Prerequisites

- PostgreSQL database (schema auto-applied on startup)
- At least one S3-compatible storage backend
- Configuration file with credentials

### Docker

Build and push a multi-arch image with a version tag:

```bash
make push VERSION=vX.Y.Z
```

The `VERSION` is baked into the binary via `-ldflags` and displayed in the web UI header and `/health` endpoint. Defaults to the value in `.version` if omitted.

### Debian Package

Build a `.deb` package for bare-metal or VM deployments:

```bash
make deb VERSION=X.Y.Z
```

Install and configure:

```bash
sudo dpkg -i s3-orchestrator_X.Y.Z_amd64.deb
sudo vim /etc/s3-orchestrator/config.yaml
sudo vim /etc/default/s3-orchestrator   # set DB_PASSWORD, backend keys, etc.
sudo systemctl start s3-orchestrator
```

The package installs:

| Path | Purpose |
|------|---------|
| `/usr/bin/s3-orchestrator` | Binary |
| `/etc/s3-orchestrator/config.yaml` | Configuration (conffile, preserved on upgrade) |
| `/etc/default/s3-orchestrator` | Environment variables for `${VAR}` expansion |
| `/usr/lib/systemd/system/s3-orchestrator.service` | Systemd unit |
| `/var/lib/s3-orchestrator/` | Data directory |

The systemd unit runs as a dedicated `s3-orchestrator` user with filesystem hardening (`ProtectSystem=strict`, `ProtectHome=yes`, `NoNewPrivileges=yes`). Config reload via `systemctl reload s3-orchestrator` sends `SIGHUP`.

### Releasing

Tag a version and push to trigger an automated GitHub Release via GoReleaser:

```bash
make release
```

This tags the current `.version` value and pushes the tag, which triggers GoReleaser to build Linux binaries (amd64 + arm64), Debian packages, and SHA256 checksums — all attached to the GitHub Release with an auto-generated changelog.

Docker images are still built manually since the private registry isn't reachable from GitHub Actions:

```bash
make push VERSION=vX.Y.Z
```

To dry-run the release locally (builds everything without publishing):

```bash
make release-local
```

## Project Structure

```
cmd/s3-orchestrator/
  main.go                    Entry point, subcommand dispatch, background tasks
  sync.go                    Sync subcommand (bucket import)
internal/
  audit/audit.go             Request ID generation, context propagation, audit logger
  auth/auth.go               BucketRegistry, SigV4 verification, legacy token auth
  config/config.go           YAML config loader with env var expansion
  server/
    server.go                HTTP router, bucket resolution, key prefixing, metrics
    objects.go               PUT, GET, HEAD, DELETE, COPY, DeleteObjects handlers
    list.go                  ListObjectsV2 handler (XML response)
    multipart.go             Multipart upload handlers
    helpers.go               Path parsing, S3 XML error responses
    ratelimit.go             Per-IP token bucket rate limiter
  storage/
    backend.go               S3 client (AWS SDK v2)
    metadata.go              MetadataStore interface, sentinel errors
    store.go                 PostgreSQL storage layer (pgx/v5 + sqlc)
    cleanup_queue.go         Cleanup queue Store methods (enqueue, retry, complete)
    circuitbreaker.go        Three-state circuit breaker wrapper
    manager.go               Multi-backend routing, quota selection, shared helpers
    manager_objects.go       Object CRUD with read failover, broadcast, batch delete
    manager_multipart.go     Multipart upload lifecycle
    manager_lifecycle.go     Lifecycle expiration rule processing
    manager_cleanup.go       Cleanup queue processing and enqueue helper
    manager_dashboard.go     DashboardData type + thin wrappers
    location_cache.go        Key→backend cache with TTL + background eviction
    usage_tracker.go         Atomic usage counters, limit enforcement, flush
    metrics_collector.go     Prometheus metric recording + gauge refresh
    dashboard_aggregator.go  Dashboard data aggregation + lazy directory listing
    rebalancer.go            Object rebalancing across backends
    replicator.go            Cross-backend object replication
    sqlc/
      schema.sql             Schema for sqlc code generation
      queries/               Annotated SQL query files
      *.go                   Generated type-safe query code (do not edit)
  ui/
    handler.go               Web UI HTTP handler, session auth + JSON APIs
    templates.go             Embedded templates + formatting helpers
    templates/dashboard.html Dashboard HTML template
    templates/login.html     Login page HTML template
    static/style.css         Dashboard stylesheet
    static/tree.js           Lazy-loaded directory tree, file management, sync/rebalance
  testutil/
    mock_store.go            Shared MetadataStore mock for tests
  telemetry/
    metrics.go               Prometheus metric definitions
    tracing.go               OpenTelemetry tracer setup
grafana/
  s3-orchestrator.json       Grafana dashboard (all Prometheus metrics)
sqlc.yaml                    sqlc configuration
Dockerfile                   Multi-stage build
Makefile                     Build, test, lint, generate, push, deb targets
nfpm.yaml                    Debian package definition (nfpm)
packaging/
  s3-orchestrator.service    Systemd unit file
  config.yaml                Sample config installed to /etc/s3-orchestrator/
  s3-orchestrator.default    Default env file installed to /etc/default/
  preinstall.sh              Creates system user and directories
  postinstall.sh             Enables systemd service
  postremove.sh              Purge cleanup (removes user and data)
  changelog                  Debian changelog
  copyright                  Debian copyright file
  lintian-overrides          Lintian override rules
config.example.yaml          Configuration reference
```
